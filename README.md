# üåä Sistema de Datos de Embalses Espa√±a

Backend que descarga autom√°ticamente los datos oficiales del MITECO
y los expone como una API REST para tu web.

---

## üìÅ Estructura del proyecto

```
embalses/
‚îú‚îÄ‚îÄ fetch_embalses.py   # Script de descarga y parseo
‚îú‚îÄ‚îÄ api.py              # API REST con Flask
‚îú‚îÄ‚îÄ embalses.db         # Base de datos SQLite (se genera autom√°ticamente)
‚îú‚îÄ‚îÄ fetch.log           # Log de ejecuciones
‚îú‚îÄ‚îÄ requirements.txt    # Dependencias Python
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Instalaci√≥n

### 1. Instalar dependencias

```bash
python -m venv venv
source venv/bin/activate        # Linux/Mac
# venv\Scripts\activate         # Windows

pip install -r requirements.txt
```

### 2. Primera descarga de datos

```bash
python fetch_embalses.py
```

Esto descargar√° el ZIP del MITECO (~50 MB) y crear√° `embalses.db`.

### 3. Arrancar la API

```bash
# Desarrollo
python api.py

# Producci√≥n (con gunicorn)
gunicorn -w 4 -b 0.0.0.0:5000 "api:app"
```

---

## ‚è∞ Cron Job (actualizaci√≥n autom√°tica cada martes)

El MITECO publica datos nuevos cada martes. Para automatizar la descarga:

### Linux/Mac

```bash
# Abrir el editor de cron
crontab -e

# A√±adir esta l√≠nea (ejecuta cada martes a las 10:00 AM):
0 10 * * 2 /ruta/a/venv/bin/python /ruta/a/embalses/fetch_embalses.py >> /ruta/a/embalses/fetch.log 2>&1
```

**Ejemplo con rutas reales:**
```
0 10 * * 2 /home/usuario/embalses/venv/bin/python /home/usuario/embalses/fetch_embalses.py >> /home/usuario/embalses/fetch.log 2>&1
```

### Windows (Task Scheduler)

```powershell
# Crear tarea programada en PowerShell (ejecutar como admin):
$action = New-ScheduledTaskAction -Execute "C:\ruta\venv\Scripts\python.exe" -Argument "C:\ruta\embalses\fetch_embalses.py"
$trigger = New-ScheduledTaskTrigger -Weekly -DaysOfWeek Tuesday -At 10:00AM
Register-ScheduledTask -Action $action -Trigger $trigger -TaskName "EmbalsesMITECO" -RunLevel Highest
```

---

## üîå Endpoints de la API

| M√©todo | Endpoint | Descripci√≥n |
|--------|----------|-------------|
| GET | `/api/meta` | Info √∫ltima actualizaci√≥n |
| GET | `/api/resumen` | Estad√≠sticas nacionales |
| GET | `/api/cuencas` | Lista de cuencas |
| GET | `/api/embalses` | Todos los embalses (√∫ltima semana) |
| GET | `/api/embalses?cuenca=Tajo` | Filtrar por cuenca |
| GET | `/api/embalses?min_porc=50` | Por % m√≠nimo de llenado |
| GET | `/api/embalses/<nombre>` | Detalle + hist√≥rico |
| GET | `/api/embalses/<nombre>?desde=2020-01-01` | Hist√≥rico con fechas |

### Ejemplo de respuesta `/api/embalses`

```json
{
  "total": 412,
  "page": 1,
  "per_page": 50,
  "pages": 9,
  "data": [
    {
      "nombre": "Alc√°ntara",
      "cuenca": "Tajo",
      "comunidad": "Extremadura",
      "capacidad_hm3": 3162.0,
      "volumen_hm3": 1890.4,
      "porcentaje": 59.8,
      "fecha": "2026-02-18"
    },
    ...
  ]
}
```

---

## üåê Uso desde tu web frontend

```javascript
// Obtener todos los embalses de la √∫ltima semana
const resp = await fetch('http://localhost:5000/api/embalses');
const { data } = await resp.json();

// Filtrar por cuenca
const tajo = await fetch('http://localhost:5000/api/embalses?cuenca=Tajo');

// Resumen nacional
const resumen = await fetch('http://localhost:5000/api/resumen');
// ‚Üí { total_embalses: 412, porcentaje_medio: 58.3, capacidad_total_hm3: 52847.0, ... }
```

---

## ‚òÅÔ∏è Despliegue en producci√≥n

### Opci√≥n A: VPS propio (Nginx + Gunicorn)

```nginx
# /etc/nginx/sites-available/embalses
server {
    listen 80;
    server_name tu-dominio.com;

    location /api/ {
        proxy_pass http://127.0.0.1:5000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

### Opci√≥n B: Railway / Render / Fly.io (gratis)

Sube el proyecto a GitHub y despli√©galo directamente. A√±ade un `Procfile`:

```
web: gunicorn -w 2 -b 0.0.0.0:$PORT "api:app"
```

Y configura el cron job con el scheduler integrado de cada plataforma.

---

## üìù Notas

- Los datos del MITECO tienen un retardo de ~1 semana (datos provisionales).
- El fichero ZIP puede pesar ~50 MB; el script solo necesita ejecutarse una vez por semana.
- La base de datos SQLite resultante ocupa ~20-40 MB con datos desde 1988.
- **Fuente oficial:** https://www.miteco.gob.es/es/agua/temas/evaluacion-de-los-recursos-hidricos/boletin-hidrologico.html
